{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval 1#\n",
    "## Assignment 2: Retrieval models [100 points] ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you will get familiar with basic and advanced information retrieval concepts. You will implement different information retrieval ranking models and evaluate their performance.\n",
    "\n",
    "We provide you with a Indri index. To query the index, you'll use a Python package ([pyndri](https://github.com/cvangysel/pyndri)) that allows easy access to the underlying document statistics.\n",
    "\n",
    "For evaluation you'll use the [TREC Eval](https://github.com/usnistgov/trec_eval) utility, provided by the National Institute of Standards and Technology of the United States. TREC Eval is the de facto standard way to compute Information Retrieval measures and is frequently referenced in scientific papers.\n",
    "\n",
    "This is a **groups-of-three assignment**, the deadline is **Wednesday, January 31st**. Code quality, informative comments and convincing analysis of the results will be considered when grading. Submission should be done through blackboard, questions can be asked on the course [Piazza](piazza.com/university_of_amsterdam/spring2018/52041inr6y/home).\n",
    "\n",
    "### Technicalities (must-read!) ###\n",
    "\n",
    "The assignment directory is organized as follows:\n",
    "   * `./assignment.ipynb` (this file): the description of the assignment.\n",
    "   * `./index/`: the index we prepared for you.\n",
    "   * `./ap_88_90/`: directory with ground-truth and evaluation sets:\n",
    "      * `qrel_test`: test query relevance collection (**test set**).\n",
    "      * `qrel_validation`: validation query relevance collection (**validation set**).\n",
    "      * `topics_title`: semicolon-separated file with query identifiers and terms.\n",
    "\n",
    "You will need the following software packages (tested with Python 3.5 inside [Anaconda](https://conda.io/docs/user-guide/install/index.html)):\n",
    "   * Python 3.5 and Jupyter\n",
    "   * Indri + Pyndri (Follow the installation instructions [here](https://github.com/nickvosk/pyndri/blob/master/README.md))\n",
    "   * gensim [link](https://radimrehurek.com/gensim/install.html)\n",
    "   * TREC Eval [link](https://github.com/usnistgov/trec_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TREC Eval primer ###\n",
    "The TREC Eval utility can be downloaded and compiled as follows:\n",
    "\n",
    "    git clone https://github.com/usnistgov/trec_eval.git\n",
    "    cd trec_eval\n",
    "    make\n",
    "\n",
    "TREC Eval computes evaluation scores given two files: ground-truth information regarding relevant documents, named *query relevance* or *qrel*, and a ranking of documents for a set of queries, referred to as a *run*. The *qrel* will be supplied by us and should not be changed. For every retrieval model (or combinations thereof) you will generate a run of the top-1000 documents for every query. The format of the *run* file is as follows:\n",
    "\n",
    "    $query_identifier Q0 $document_identifier $rank_of_document_for_query $query_document_similarity $run_identifier\n",
    "    \n",
    "where\n",
    "   * `$query_identifier` is the unique identifier corresponding to a query (usually this follows a sequential numbering).\n",
    "   * `Q0` is a legacy field that you can ignore.\n",
    "   * `$document_identifier` corresponds to the unique identifier of a document (e.g., APXXXXXXX where AP denotes the collection and the Xs correspond to a unique numerical identifier).\n",
    "   * `$rank_of_document_for_query` denotes the rank of the document for the particular query. This field is ignored by TREC Eval and is only maintained for legacy support. The ranks are computed by TREC Eval itself using the `$query_document_similarity` field (see next). However, it remains good practice to correctly compute this field.\n",
    "   * `$query_document_similarity` is a score indicating the similarity between query and document where a higher score denotes greater similarity.\n",
    "   * `$run_identifier` is an identifier of the run. This field is for your own convenience and has no purpose beyond bookkeeping.\n",
    "   \n",
    "For example, say we have two queries: `Q1` and `Q2` and we rank three documents (`DOC1`, `DOC2`, `DOC3`). For query `Q1`, we find the following similarity scores `score(Q1, DOC1) = 1.0`, `score(Q1, DOC2) = 0.5`, `score(Q1, DOC3) = 0.75`; and for `Q2`: `score(Q2, DOC1) = -0.1`, `score(Q2, DOC2) = 1.25`, `score(Q1, DOC3) = 0.0`. We can generate run using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q1 Q0 DOC1 1 1.0 example\n",
      "Q1 Q0 DOC3 2 0.75 example\n",
      "Q1 Q0 DOC2 3 0.5 example\n",
      "Q2 Q0 DOC2 1 1.25 example\n",
      "Q2 Q0 DOC3 2 0.0 example\n",
      "Q2 Q0 DOC1 3 -0.1 example\n"
     ]
    }
   ],
   "source": [
    "# STD\n",
    "import collections\n",
    "import io\n",
    "import logging\n",
    "from math import log\n",
    "import sys\n",
    "import time\n",
    "import os\n",
    "\n",
    "# EXT\n",
    "import numpy as np\n",
    "import pyndri\n",
    "\n",
    "# PROJECT\n",
    "from embeddings import doc_kmeans, doc_centroid, doc_tfidf_scaling, VectorCollection, load_document_representations\n",
    "from kernels import k_passage, k_gaussian, k_cosine, k_triangle, k_circle\n",
    "from lsm import LSM\n",
    "from plm import PLM\n",
    "from LTR import get_dataset_for_features\n",
    "from models import LinearRanker, train\n",
    "\n",
    "# GLOBALS\n",
    "rankings = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "\n",
    "\n",
    "def write_run(model_name, data, out_f,\n",
    "              max_objects_per_query=sys.maxsize,\n",
    "              skip_sorting=False):\n",
    "    \"\"\"\n",
    "    Write a run to an output file.\n",
    "    Parameters:\n",
    "        - model_name: identifier of run.\n",
    "        - data: dictionary mapping topic_id to object_assesments;\n",
    "            object_assesments is an iterable (list or tuple) of\n",
    "            (relevance, object_id) pairs.\n",
    "            The object_assesments iterable is sorted by decreasing order.\n",
    "        - out_f: output file stream.\n",
    "        - max_objects_per_query: cut-off for number of objects per query.\n",
    "    \"\"\"\n",
    "    for subject_id, object_assesments in data.items():\n",
    "        if not object_assesments:\n",
    "            logging.warning('Received empty ranking for %s; ignoring.',\n",
    "                            subject_id)\n",
    "\n",
    "            continue\n",
    "\n",
    "        # Probe types, to make sure everything goes alright.\n",
    "        # assert isinstance(object_assesments[0][0], float) or \\\n",
    "        #     isinstance(object_assesments[0][0], np.float32)\n",
    "        assert isinstance(object_assesments[0][1], str) or \\\n",
    "            isinstance(object_assesments[0][1], bytes)\n",
    "\n",
    "        if not skip_sorting:\n",
    "            object_assesments = sorted(object_assesments, reverse=True)\n",
    "\n",
    "        if max_objects_per_query < sys.maxsize:\n",
    "            object_assesments = object_assesments[:max_objects_per_query]\n",
    "\n",
    "        if isinstance(subject_id, bytes):\n",
    "            subject_id = subject_id.decode('utf8')\n",
    "\n",
    "        for rank, (relevance, object_id) in enumerate(object_assesments):\n",
    "            if isinstance(object_id, bytes):\n",
    "                object_id = object_id.decode('utf8')\n",
    "\n",
    "            out_f.write(\n",
    "                '{subject} Q0 {object} {rank} {relevance} '\n",
    "                '{model_name}\\n'.format(\n",
    "                    subject=subject_id,\n",
    "                    object=object_id,\n",
    "                    rank=rank + 1,\n",
    "                    relevance=relevance,\n",
    "                    model_name=model_name))\n",
    "            \n",
    "# The following writes the run to standard output.\n",
    "# In your code, you should write the runs to local\n",
    "# storage in order to pass them to trec_eval.\n",
    "write_run(\n",
    "    model_name='example',\n",
    "    data={\n",
    "        'Q1': ((1.0, 'DOC1'), (0.5, 'DOC2'), (0.75, 'DOC3')),\n",
    "        'Q2': ((-0.1, 'DOC1'), (1.25, 'DOC2'), (0.0, 'DOC3')),\n",
    "    },\n",
    "    out_f=sys.stdout,\n",
    "    max_objects_per_query=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, imagine that we know that `DOC1` is relevant and `DOC3` is non-relevant for `Q1`. In addition, for `Q2` we only know of the relevance of `DOC3`. The query relevance file looks like:\n",
    "\n",
    "    Q1 0 DOC1 1\n",
    "    Q1 0 DOC3 0\n",
    "    Q2 0 DOC3 1\n",
    "    \n",
    "We store the run and qrel in files `example.run` and `example.qrel` respectively on disk. We can now use TREC Eval to compute evaluation measures. In this example, we're only interested in Mean Average Precision and we'll only show this below for brevity. However, TREC Eval outputs much more information such as NDCG, recall, precision, etc.\n",
    "\n",
    "    $ trec_eval -m all_trec -q example.qrel example.run | grep -E \"^map\\s\"\n",
    "    > map                   \tQ1\t1.0000\n",
    "    > map                   \tQ2\t0.5000\n",
    "    > map                   \tall\t0.7500\n",
    "    \n",
    "Now that we've discussed the output format of rankings and how you can compute evaluation measures from these rankings, we'll now proceed with an overview of the indexing framework you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pyndri primer ###\n",
    "For this assignment you will use [Pyndri](https://github.com/cvangysel/pyndri) [[1](https://arxiv.org/abs/1701.00749)], a python interface for [Indri](https://www.lemurproject.org/indri.php). We have indexed the document collection and you can query the index using Pyndri. We will start by giving you some examples of what Pyndri can do:\n",
    "\n",
    "First we read the document collection index with Pyndri:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parse_topics(file_or_files,\n",
    "                 max_topics=sys.maxsize, delimiter=';'):\n",
    "    assert max_topics >= 0 or max_topics is None\n",
    "\n",
    "    topics = collections.OrderedDict()\n",
    "\n",
    "    if not isinstance(file_or_files, list) and \\\n",
    "            not isinstance(file_or_files, tuple):\n",
    "        if hasattr(file_or_files, '__iter__'):\n",
    "            file_or_files = list(file_or_files)\n",
    "        else:\n",
    "            file_or_files = [file_or_files]\n",
    "\n",
    "    for f in file_or_files:\n",
    "        assert isinstance(f, io.IOBase)\n",
    "\n",
    "        for line in f:\n",
    "            assert(isinstance(line, str))\n",
    "\n",
    "            line = line.strip()\n",
    "\n",
    "            if not line:\n",
    "                continue\n",
    "\n",
    "            topic_id, terms = line.split(delimiter, 1)\n",
    "\n",
    "            if topic_id in topics and (topics[topic_id] != terms):\n",
    "                    logging.error('Duplicate topic \"%s\" (%s vs. %s).',\n",
    "                                  topic_id,\n",
    "                                  topics[topic_id],\n",
    "                                  terms)\n",
    "\n",
    "            topics[topic_id] = terms\n",
    "\n",
    "            if max_topics > 0 and len(topics) >= max_topics:\n",
    "                break\n",
    "\n",
    "    return topics\n",
    "\n",
    "\n",
    "# ------------------------\n",
    "#  Commonly used functions\n",
    "# ------------------------\n",
    "\n",
    "def create_index_resources(index_path=\"index/\"):\n",
    "    index = pyndri.Index(index_path)\n",
    "    token2id, id2token, id2df = index.get_dictionary()\n",
    "    dictionary = pyndri.extract_dictionary(index)\n",
    "    document_ids = list(range(index.document_base(), index.maximum_document()))\n",
    "    return index, token2id, id2token, id2df, dictionary, document_ids\n",
    "\n",
    "\n",
    "def create_query_resources(query_path='./ap_88_89/topics_title'):\n",
    "    with open(query_path, 'r') as f_topics:\n",
    "        queries = parse_topics([f_topics])\n",
    "\n",
    "    tokenized_queries = {\n",
    "        query_id: [dictionary.translate_token(token)\n",
    "                   for token in index.tokenize(query_string)\n",
    "                   if dictionary.has_token(token)]\n",
    "        for query_id, query_string in queries.items()}\n",
    "\n",
    "    # Record in which query specific query terms are occurring (inverted index)\n",
    "    query_terms_inverted = collections.defaultdict(set)\n",
    "    for query_id, query_term_ids in tokenized_queries.items():\n",
    "        for query_term_id in query_term_ids:\n",
    "            # A lookup-table for what queries this term appears in\n",
    "            query_terms_inverted[query_term_id].add(int(query_id))\n",
    "\n",
    "    query_term_ids = set(\n",
    "        query_term_id\n",
    "        for query_term_ids in tokenized_queries.values()\n",
    "        for query_term_id in query_term_ids)\n",
    "\n",
    "    return queries, tokenized_queries, query_terms_inverted, query_term_ids\n",
    "\n",
    "\n",
    "def build_misc_resources(document_ids, query_terms_inverted):\n",
    "    # Inverted Index creation.\n",
    "    start_time = time.time()\n",
    "\n",
    "    inverted_index = collections.defaultdict(lambda: collections.defaultdict(int))\n",
    "    tf_C = collections.Counter()\n",
    "    query_word_positions = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "    document_lengths = {}\n",
    "    unique_terms_per_document = {}\n",
    "    collection_length = 0\n",
    "    total_terms = 0\n",
    "\n",
    "    for int_doc_id in document_ids:\n",
    "        ext_doc_id, doc_token_ids = index.document(int_doc_id)\n",
    "\n",
    "        for pos, id_at_pos in enumerate(doc_token_ids):\n",
    "            if id_at_pos in query_term_ids:\n",
    "                for query_id in query_terms_inverted[id_at_pos]:\n",
    "                    query_word_positions[int_doc_id][int(query_id)].append(pos)\n",
    "\n",
    "        document_bow = collections.Counter(\n",
    "            token_id for token_id in doc_token_ids\n",
    "            if token_id > 0\n",
    "        )\n",
    "        document_length = sum(document_bow.values())\n",
    "\n",
    "        collection_length += len(doc_token_ids)\n",
    "\n",
    "        document_lengths[int_doc_id] = document_length\n",
    "        total_terms += document_length\n",
    "\n",
    "        unique_terms_per_document[int_doc_id] = len(document_bow)\n",
    "\n",
    "        for query_term_id in query_term_ids:\n",
    "            assert query_term_id is not None\n",
    "\n",
    "            document_term_frequency = document_bow.get(query_term_id, 0)\n",
    "\n",
    "            if document_term_frequency == 0:\n",
    "                continue\n",
    "\n",
    "            inverted_index[query_term_id][int_doc_id] = document_term_frequency\n",
    "            tf_C[query_term_id] += document_term_frequency\n",
    "\n",
    "    print('Inverted index creation took', time.time() - start_time, 'seconds.')\n",
    "    print(\"Done creating tf_c and query_word_positions.\")\n",
    "    avg_doc_length = total_terms / len(document_ids)\n",
    "    return inverted_index, tf_C, query_word_positions, unique_terms_per_document, avg_doc_length, document_length, \\\n",
    "           collection_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loaded index can be used to access a collection of documents in an easy manner. We'll give you some examples to get some idea of what it can do, it is up to you to figure out how to use it for the remainder of the assignment.\n",
    "\n",
    "First let's look at the number of documents, since Pyndri indexes the documents using incremental identifiers we can simply take the lowest index and the maximum document and consider the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We removed the example code to clean up the code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the first document out of the collection and take a look at it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a document consists of two things, a string representing the external document identifier and an integer list representing the identifiers of words that make up the document. Pyndri uses integer representations for words or terms, thus a token_id is an integer that represents a word whereas the token is the actual text of the word/term. Every id has a unique token and vice versa with the exception of stop words: words so common that there are uninformative, all of these receive the zero id.\n",
    "\n",
    "To see what some ids and their matching tokens we take a look at the dictionary of the index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this dictionary we can see the tokens for the (non-stop) words in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse can also be done, say we want to look for news about the \"University of Massachusetts\", the tokens of that query can be converted to ids using the reverse dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally we can now match the document and query in the id space, let's see how often a word from the query occurs in our example document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this is certainly not everything Pyndri can do, it should give you an idea of how to use it. Please take a look at the [examples](https://github.com/cvangysel/pyndri) as it will help you a lot with this assignment.\n",
    "\n",
    "**CAUTION**: Avoid printing out the whole index in this Notebook as it will generate a lot of output and is likely to corrupt the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing the query file\n",
    "You can parse the query file (`ap_88_89/topics_title`) using the following snippet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Implement and compare lexical IR methods [35 points] ### \n",
    "\n",
    "In this task you will implement a number of lexical methods for IR using the **Pyndri** framework. Then you will evaluate these methods on the dataset we have provided using **TREC Eval**.\n",
    "\n",
    "Use the **Pyndri** framework to get statistics of the documents (term frequency, document frequency, collection frequency; **you are not allowed to use the query functionality of Pyndri**) and implement the following scoring methods in **Python**:\n",
    "\n",
    "- [TF-IDF](http://nlp.stanford.edu/IR-book/html/htmledition/tf-idf-weighting-1.html) and \n",
    "- [BM25](http://nlp.stanford.edu/IR-book/html/htmledition/okapi-bm25-a-non-binary-model-1.html) with k1=1.2 and b=0.75. **[5 points]**\n",
    "- Language models ([survey](https://drive.google.com/file/d/0B-zklbckv9CHc0c3b245UW90NE0/view))\n",
    "    - Jelinek-Mercer (explore different values of ð›Œ in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - Dirichlet Prior (explore different values of ð› [500, 1000, 1500]). **[5 points]**\n",
    "    - Absolute discounting (explore different values of ð›… in the range [0.1, 0.5, 0.9]). **[5 points]**\n",
    "    - [Positional Language Models](http://sifaka.cs.uiuc.edu/~ylv2/pub/sigir09-plm.pdf) define a language model for each position of a document, and score a document based on the scores of its PLMs. The PLM is estimated based on propagated counts of words within a document through a proximity-based density function, which both captures proximity heuristics and achieves an effect of â€œsoftâ€ passage retrieval. Implement the PLM, all five kernels, but only the Best position strategy to score documents. Use ð›” equal to 50, and Dirichlet smoothing with ð› optimized on the validation set (decide how to optimize this value yourself and motivate your decision in the report). **[10 points]**\n",
    "    \n",
    "Implement the above methods and report evaluation measures (on the test set) using the hyper parameter values you optimized on the validation set (also report the values of the hyper parameters). Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "For the language models, create plots showing `NDCG@10` with varying values of the parameters. You can do this by chaining small scripts using shell scripting (preferred) or execute trec_eval using Python's `subprocess`.\n",
    "\n",
    "Compute significance of the results using a [two-tailed paired Student t-test](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ttest_rel.html) **[5 points]**. Be wary of false rejection of the null hypothesis caused by the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). There are multiple ways to mitigate this problem and it is up to you to choose one.\n",
    "\n",
    "Analyse the results by identifying specific queries where different methods succeed or fail and discuss possible reasons that cause these differences. This is *very important* in order to understand who the different retrieval functions behave.\n",
    "\n",
    "**NOTE**: Donâ€™t forget to use log computations in your calculations to avoid underflows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT**: You should structure your code around the helper functions we provide below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# Task 1: Implement and compare lexical IR methods\n",
    "# ------------------------------------------------\n",
    "\n",
    "\n",
    "def run_retrieval(index, model_name, queries, document_ids, scoring_func, max_objects_per_query=1000,\n",
    "                  target_dir=\"./lexical_results/\", **resource_params):\n",
    "    \"\"\"\n",
    "    Runs a retrieval method for all the queries and writes the TREC-friendly results in a file.\n",
    "\n",
    "    :param index: Pyndri index object.\n",
    "    :type index: pyndri.Index\n",
    "    :param model_name: Name of the model (used for screen printing).\n",
    "    :type model_name: str\n",
    "    :param queries: Queries to be evaluated.\n",
    "    :type queries: dict\n",
    "    :param document_ids: Collection of document ids in data set.\n",
    "    :type document_ids: list or set or tuple\n",
    "    :param scoring_func: Function that uses the prepared resources, query and document id to score a query and a doc.\n",
    "    :type scoring_func: func\n",
    "    :param max_objects_per_query: Only keep a certain number of best ranked documents per query.\n",
    "    :type max_objects_per_query: int\n",
    "    :param resource_params: Named arguments that are used to build resources used by the model for scoring.\n",
    "    :type resource_params: dict\n",
    "    \"\"\"\n",
    "    # The dictionary data should have the form: model_name --> query_id --> (document_score, external_doc_id)\n",
    "    global rankings, reranking_ids\n",
    "    run_out_path = '{}.run'.format(model_name)\n",
    "\n",
    "    run_id = 0\n",
    "    while os.path.exists(run_out_path):\n",
    "        run_id += 1\n",
    "        run_out_path = '{}_{}.run'.format(model_name, run_id)\n",
    "\n",
    "    print('Retrieving using', model_name)\n",
    "    start = time.time()\n",
    "    query_times = 0\n",
    "    n_queries = len(queries)\n",
    "\n",
    "    for i, query in enumerate(queries.items()):\n",
    "        query_start_time = time.time()\n",
    "        query_id, _ = query\n",
    "        query_scores = []\n",
    "\n",
    "        # Do actual scoring here\n",
    "        for n, document_id in enumerate(document_ids):\n",
    "            ext_doc_id, document_word_positions = index.document(document_id)\n",
    "            score = scoring_func(index, query_id, document_id, **resource_params)\n",
    "            query_scores.append((score, ext_doc_id))\n",
    "\n",
    "        query_id = int(query_id)\n",
    "        rankings[model_name][query_id] = list(sorted(query_scores, reverse=True))[:max_objects_per_query]\n",
    "\n",
    "        query_end_time = time.time()\n",
    "        query_time = query_end_time - query_start_time\n",
    "        query_times += query_time\n",
    "        average_query_time = query_times / max(i, 1)\n",
    "        querys_left = len(queries) - i\n",
    "        time_left = average_query_time * querys_left\n",
    "        m, s = divmod(time_left, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "        print(\n",
    "            \"\\rAverage query time for query {} out of {}: {:.2f} seconds. {} hour(s), {} minute(s) and {:.2f} seconds \"\n",
    "            \"remaining for {} queries.\".format(\n",
    "                i+1, n_queries, average_query_time, int(h), int(m), s, querys_left\n",
    "            ), flush=True, end=\"\"\n",
    "        )\n",
    "\n",
    "    # Write results\n",
    "    if not os.path.exists(target_dir):\n",
    "        os.makedirs(target_dir)\n",
    "\n",
    "    with open('{}{}'.format(target_dir, run_out_path), 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=model_name,\n",
    "            data=rankings[model_name],\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=max_objects_per_query)\n",
    "\n",
    "    print(\"Done writing results to run file\")\n",
    "\n",
    "    end = time.time()\n",
    "    print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "\n",
    "def idf(term_id, id2df, num_documents):\n",
    "    df_t = id2df[term_id]\n",
    "    return log(num_documents) - log(df_t)\n",
    "\n",
    "\n",
    "def tf_idf(index, query_id, document_id, document_term_freqs, tokenized_queries, id2df, num_documents,\n",
    "           **resource_params):\n",
    "    log_sum = 0\n",
    "\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        # TODO: How are we adding the values here?\n",
    "        log_sum += log(1 + document_term_freqs[query_term_id][document_id]) * idf(query_term_id, id2df, num_documents)\n",
    "\n",
    "    return log_sum\n",
    "\n",
    "\n",
    "def bm25(index, query_id, document_id, document_term_freqs, avg_doc_length, id2df, num_documents, tokenized_queries,\n",
    "         **resource_params):\n",
    "    \"\"\"\n",
    "    :param document_id:\n",
    "    :param term_id:\n",
    "    :param document_term_freq: How many times this term appears in the given document\n",
    "    :_ unused tuning parameter\n",
    "    :returns: A score for the given document in the light of the given query term\n",
    "\n",
    "    Since all the scoring functions are supposed to only score one query term,\n",
    "    the BM25 summation is being done outside this function.\n",
    "    Do note that we leave the k_3 factor out, since all the queries\n",
    "    in this assignment can be assumed to be reasonably short.\n",
    "    \"\"\"\n",
    "\n",
    "    def bm25_formula(document_id, query_term_id, document_term_freqs, l_d, l_average, id2df, num_documents):\n",
    "        enumerator = (k_1 + 1) * document_term_freqs[query_term_id][document_id]\n",
    "        denominator = k_1 * ((1-b) + b * (l_d/l_average)) + document_term_freqs[query_term_id][document_id]\n",
    "        bm25_score = idf(query_term_id, id2df, num_documents) * enumerator/denominator\n",
    "\n",
    "        if bm25_score == 0:\n",
    "            return 0\n",
    "        return bm25_score\n",
    "\n",
    "    k_1 = 1.2\n",
    "    b = 0.75\n",
    "    l_average = avg_doc_length\n",
    "    l_d = index.document_length(document_id)\n",
    "\n",
    "    sum_ = 0\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        # TODO: How do we combine the values here?\n",
    "        sum_ += bm25_formula(document_id, query_term_id, document_term_freqs, l_d, l_average, id2df, num_documents)\n",
    "\n",
    "    return sum_\n",
    "\n",
    "\n",
    "def LM_jelinek_mercer_smoothing(index, query_id, document_id, document_term_freqs, collection_length, tf_C,\n",
    "                                tokenized_queries, tuning_parameter=0.1, **resource_params):\n",
    "    log_sum = 0\n",
    "\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        tf = document_term_freqs[query_term_id][document_id]\n",
    "        lamb = tuning_parameter\n",
    "        doc_length = index.document_length(document_id)\n",
    "        C = collection_length\n",
    "\n",
    "        try:\n",
    "            prob_q_d = lamb * (tf / doc_length) + (1 - lamb) * (tf_C[query_term_id] / C)\n",
    "        except ZeroDivisionError:\n",
    "            prob_q_d = 0\n",
    "\n",
    "        log_sum += np.log(prob_q_d)\n",
    "\n",
    "    return log_sum\n",
    "\n",
    "\n",
    "def LM_dirichlet_smoothing(index, query_id, document_id, document_term_freqs, collection_length, tokenized_queries,\n",
    "                           tuning_parameter=500, **resource_params):\n",
    "    log_sum = 0\n",
    "\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        tf = document_term_freqs[query_term_id][document_id]\n",
    "        mu = tuning_parameter\n",
    "        doc_length = index.document_length(document_id)\n",
    "        C = collection_length\n",
    "\n",
    "        prob_q_d = (tf + mu * (tf_C[query_term_id] / C)) / (doc_length + mu)\n",
    "\n",
    "        log_sum += np.log(prob_q_d)\n",
    "\n",
    "    return log_sum\n",
    "\n",
    "def LM_absolute_discounting(index, query_id, document_id, document_term_freq, num_unique_words, collection_length,\n",
    "                         tokenized_queries, tuning_parameter=0.1):\n",
    "    log_sum = 0\n",
    "\n",
    "    for query_term_id in tokenized_queries[query_id]:\n",
    "        discount = tuning_parameter\n",
    "        d = index.document_length(document_id)\n",
    "        C = collection_length\n",
    "        if d == 0: return -9999\n",
    "        number_of_unique_terms = num_unique_words[document_id]\n",
    "\n",
    "        log_sum += np.log(\n",
    "            max(document_term_freq - discount, 0) / d + ((discount * number_of_unique_terms) / d) *\n",
    "            (tf_C[query_term_id] / C)\n",
    "        )\n",
    "\n",
    "    return log_sum\n",
    "\n",
    "\n",
    "def create_all_lexical_run_files(index, document_ids, queries, document_term_freqs, collection_length, tf_C,\n",
    "                                 tokenized_queries, background_model, idf2df, num_documents):\n",
    "    print(\"##### Creating all lexical run files! #####\")\n",
    "    start = time.time()\n",
    "    print(\"Running TFIDF\")\n",
    "    run_retrieval(\n",
    "        index, 'tfidf', queries, document_ids, tf_idf,\n",
    "        document_term_freqs=document_term_freqs, tokenized_queries=tokenized_queries, id2df=idf2df,\n",
    "        num_documents=num_documents\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    start = time.time()\n",
    "    print(\"Running BM25\")\n",
    "    run_retrieval(\n",
    "        index, 'bm25', queries, document_ids, bm25,\n",
    "        document_term_freqs=document_term_freqs, avg_doc_length=avg_doc_length, id2df=id2df,\n",
    "        num_documents=num_documents, tokenized_queries=tokenized_queries\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    j_m__lambda_values = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    for val in j_m__lambda_values:\n",
    "        start = time.time()\n",
    "        print(\"Running LM_jelinek\", val)\n",
    "        run_retrieval(\n",
    "            index, 'LM_jelinek_mercer_smoothing_{}'.format(str(val).replace(\".\", \"_\")),\n",
    "            queries, document_ids, LM_jelinek_mercer_smoothing,\n",
    "            tuning_parameter=val, document_term_freqs=document_term_freqs, collection_length=collection_length,\n",
    "            tf_C=tf_C, tokenized_queries=tokenized_queries\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    dirichlet_values = [500, 1000, 1500]\n",
    "    for val in dirichlet_values:\n",
    "        start = time.time()\n",
    "        print(\"Running Dirichlet\", val)\n",
    "        run_retrieval(\n",
    "            index, 'LM_dirichelt_smoothing_{}'.format(str(val).replace(\".\", \"_\")),\n",
    "            document_ids, queries, LM_dirichlet_smoothing,\n",
    "            tuning_parameter=val, document_term_freqs=document_term_freqs, collection_length=collection_length,\n",
    "            tokenized_queries=tokenized_queries\n",
    "        )\n",
    "        end = time.time()\n",
    "        print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    absolute_discounting_values = j_m__lambda_values\n",
    "    for val in absolute_discounting_values:\n",
    "        start = time.time()\n",
    "        print(\"Running ABS_discount\", val)\n",
    "        run_retrieval('LM_absolute_discounting_{}'.format(str(val).replace(\".\", \"_\")), LM_absolute_discounting, document_ids, tuning_parameter=val)\n",
    "        end = time.time()\n",
    "        print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    start = time.time()\n",
    "    run_retrieval_plm(\n",
    "        index, 'PLM_passage', queries, document_ids, query_word_positions, background_model, tokenized_queries,\n",
    "        collection_length, kernel=k_passage\n",
    "    )\n",
    "\n",
    "    start = time.time()\n",
    "    run_retrieval_plm(\n",
    "        index, 'PLM_gaussian', queries, document_ids, query_word_positions, background_model, tokenized_queries,\n",
    "        collection_length, kernel=k_gaussian\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    start = time.time()\n",
    "    run_retrieval_plm(\n",
    "        index, 'PLM_triangle', queries, document_ids, query_word_positions, background_model, tokenized_queries,\n",
    "        collection_length, kernel=k_triangle\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    start = time.time()\n",
    "    run_retrieval_plm(\n",
    "        index, 'PLM_cosine', queries, document_ids, query_word_positions, background_model, tokenized_queries,\n",
    "        collection_length, kernel=k_cosine\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"Retrieval took {:.2f} seconds.\".format(end-start))\n",
    "\n",
    "    start = time.time()\n",
    "    run_retrieval_plm(\n",
    "        index, 'PLM_circle', queries, document_ids, query_word_positions, background_model, tokenized_queries,\n",
    "        collection_length, kernel=k_circle\n",
    "    )\n",
    "    end = time.time()\n",
    "    print(\"Retrieval took {:.2f} seconds.\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Latent Semantic Models (LSMs) [15 points] ###\n",
    "\n",
    "In this task you will experiment with applying distributional semantics methods ([LSI](http://lsa3.colorado.edu/papers/JASIS.lsi.90.pdf) **[5 points]** and [LDA](https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf) **[5 points]**) for retrieval.\n",
    "\n",
    "You do not need to implement LSI or LDA on your own. Instead, you can use [gensim](http://radimrehurek.com/gensim/index.html). An example on how to integrate Pyndri with Gensim for word2vec can be found [here](https://github.com/cvangysel/pyndri/blob/master/examples/word2vec.py). For the remaining latent vector space models, you will need to implement connector classes (such as `IndriSentences`) by yourself.\n",
    "\n",
    "In order to use a latent semantic model for retrieval, you need to:\n",
    "   * build a representation of the query **q**,\n",
    "   * build a representation of the document **d**,\n",
    "   * calculate the similarity between **q** and **d** (e.g., cosine similarity, KL-divergence).\n",
    "     \n",
    "The exact implementation here depends on the latent semantic model you are using. \n",
    "   \n",
    "Each of these LSMs come with various hyperparameters to tune. Make a choice on the parameters, and explicitly mention the reasons that led you to these decisions. You can use the validation set to optimize hyper parameters you see fit; motivate your decisions. In addition, mention clearly how the query/document representations were constructed for each LSM and explain your choices.\n",
    "\n",
    "In this experiment, you will first obtain an initial top-1000 ranking for each query using TF-IDF in **Task 1**, and then re-rank the documents using the LSMs. Use TREC Eval to obtain the results and report on `NDCG@10`, Mean Average Precision (`MAP@1000`), `Precision@5` and `Recall@1000`.\n",
    "\n",
    "Perform significance testing **[5 points]** (similar as in Task 1) in the class of semantic matching methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Task 2: Latent Semantic Models\n",
    "# ------------------------------\n",
    "\n",
    "def lsm_reranking(ranked_queries, LSM_model):\n",
    "    reranking = collections.defaultdict(list)\n",
    "    LSM_model.similarity_index.num_best = None\n",
    "\n",
    "    for query_id, docs in ranked_queries.items():\n",
    "        query_terms = [id2token[term_id] for term_id in tokenized_queries[query_id]]\n",
    "        query_bow = LSM_model.dictionary.doc2bow(query_terms)\n",
    "        query_lsm = LSM_model.model[query_bow]\n",
    "\n",
    "        # Get similarity of query with all documents\n",
    "        sims = LSM_model.similarity_index[query_lsm]\n",
    "\n",
    "        for document_id in document_ids:\n",
    "            ext_doc_id, _ = index.document(document_id)\n",
    "            reranking[query_id].append((sims[document_id-1], ext_doc_id))\n",
    "\n",
    "    for query_id in reranking:\n",
    "        reranking[query_id] = list(sorted(reranking[query_id], reverse=True))[:1000]\n",
    "\n",
    "    return reranking\n",
    "\n",
    "\n",
    "def create_latent_semantic_model_runfiles():\n",
    "    global rankings\n",
    "\n",
    "    # LSI\n",
    "    start = time.time()\n",
    "    lsi = LSM('LSI', index)\n",
    "    lsi.create_model()\n",
    "    end = time.time()\n",
    "    print(\"LSI model creation took {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "    start = time.time()\n",
    "    lsi.create_similarity_index()\n",
    "    end = time.time()\n",
    "    print(\"LSI similarity index creation took {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "    start = time.time()\n",
    "    lsi_reranking = lsm_reranking(ranked_queries=rankings['tfidf'], LSM_model=lsi)\n",
    "    end = time.time()\n",
    "    print(\"LSI reranking took {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "    start = time.time()\n",
    "    run_out_path = '{}.run'.format('LSI')\n",
    "    with open('./lexical_results/{}'.format(run_out_path), 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name='LSI',\n",
    "            data=lsi_reranking,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "    end = time.time()\n",
    "    print(\"LSI run file creation {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "    # LDA\n",
    "    start = time.time()\n",
    "    lda = LSM('LDA', index)\n",
    "    lda.create_model()\n",
    "    end = time.time()\n",
    "    print(\"LDA model creation took {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "    start = time.time()\n",
    "    lda.create_similarity_index()\n",
    "    end = time.time()\n",
    "    print(\"LDA similarity index creation took {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "    start = time.time()\n",
    "    lda_reranking = lsm_reranking(ranked_queries=rankings['tfidf'], LSM_model=lda)\n",
    "    end = time.time()\n",
    "    print(\"LDA reranking took {:.2f} seconds.\".format(end - start))\n",
    "\n",
    "    start = time.time()\n",
    "    run_out_path = '{}.run'.format('LDA')\n",
    "    with open('./lexical_results/{}'.format(run_out_path), 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name='LDA',\n",
    "            data=lda_reranking,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "    end = time.time()\n",
    "    print(\"LDA run file creation {:.2f} seconds.\".format(end - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3:  Word embeddings for ranking [20 points] (open-ended) ###\n",
    "\n",
    "First create word embeddings on the corpus we provided using [word2vec](http://arxiv.org/abs/1411.2738) -- [gensim implementation](https://radimrehurek.com/gensim/models/word2vec.html). You should extract the indexed documents using pyndri and provide them to gensim for training a model (see example [here](https://github.com/nickvosk/pyndri/blob/master/examples/word2vec.py)).\n",
    "   \n",
    "This is an open-ended task. It is left up you to decide how you will combine word embeddings to derive query and document representations. Note that since we provide the implementation for training word2vec, you will be graded based on your creativity on combining word embeddings for building query and document representations.\n",
    "\n",
    "Note: If you want to experiment with pre-trained word embeddings on a different corpus, you can use the word embeddings we provide alongside the assignment (./data/reduced_vectors_google.txt.tar.gz). These are the [google word2vec word embeddings](https://code.google.com/archive/p/word2vec/), reduced to only the words that appear in the document collection we use in this assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named 'h5py'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1a77057bd52a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Task 3 Word Embeddings for Ranking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# -----------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named 'h5py'"
     ]
    }
   ],
   "source": [
    "# -----------------------------------\n",
    "# Task 3 Word Embeddings for Ranking\n",
    "# -----------------------------------\n",
    "import h5py\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def doc_centroid(vectors, cache=None, **kwargs):\n",
    "    \"\"\" Calculate the centroid of some vectors. \"\"\"\n",
    "    return np.add.reduce(vectors) / len(vectors) if len(vectors) > 0 else np.array([]), cache\n",
    "\n",
    "\n",
    "def _doc_kmeans(vectors, k=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Helper function for doc_kmeans.\n",
    "    Calculate the centroid of vectors after clustering them using k-means and only keeping those which belong to the\n",
    "    biggest cluster. If k is None, it will be chosen using a heuristic.\n",
    "    \"\"\"\n",
    "    # Simple cases\n",
    "    if len(vectors) == 0:\n",
    "        return np.array([])\n",
    "    if len(vectors) == 1:\n",
    "        return vectors\n",
    "\n",
    "    def most_common(lst):\n",
    "        data = Counter(lst)\n",
    "        return data.most_common(1)[0][0]\n",
    "\n",
    "    # Dirty but fast heuristic\n",
    "    if k is None:\n",
    "        k = int(np.sqrt(len(vectors) / 2))\n",
    "\n",
    "    kmeans = KMeans(n_clusters=k).fit(vectors)\n",
    "    labels = kmeans.labels_\n",
    "    biggest_cluster = most_common(labels)  # Get label of biggest cluster\n",
    "\n",
    "    # Return the vectors belonging to it\n",
    "    return [vector for vector, label in zip(vectors, labels) if label == biggest_cluster]\n",
    "\n",
    "\n",
    "def doc_kmeans(vectors, k=None, cache=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the centroid of vectors after clustering them using k-means and only keeping those which belong to the\n",
    "    biggest cluster. If k is None, it will be chosen using a heuristic.\n",
    "    \"\"\"\n",
    "    filtered_vectors = _doc_kmeans(vectors, k)\n",
    "\n",
    "    # Cache most recent vectors for doc_kmeans_tfidf so you don't have to do it twice\n",
    "    if cache is not None:\n",
    "        cache[\"kmeans_filtered\"] = filtered_vectors\n",
    "    return doc_centroid(filtered_vectors), cache  # Calculate the centroid with vector from biggest cluster.\n",
    "\n",
    "\n",
    "def doc_tfidf_scaling(vectors, token_ids, document_term_freqs, id2df, number_of_documents, document_id, cache=dict(),\n",
    "                      **kwargs):\n",
    "    \"\"\"\n",
    "    Calculate the centroid of a cluster, but scale the vectors by their tf-idf value.\n",
    "    \"\"\"\n",
    "    # Easy case, empty document\n",
    "    if len(vectors) == 0:\n",
    "        return np.array([])\n",
    "\n",
    "    for i, (vector, token_id) in enumerate(zip(vectors, token_ids)):\n",
    "        # Use caching\n",
    "        if hasattr(cache, \"tfidf\"):\n",
    "            tf_idf_values = cache[\"tfidf\"]\n",
    "        else:\n",
    "            tf_idf_values = cache[\"tfidf\"] = dict()\n",
    "        term_tf_idf = tf_idf_values.get(\n",
    "            (token_id, document_id),\n",
    "            tf_idf(token_id, document_term_freqs[token_id][document_id], id2df, number_of_documents)\n",
    "        )\n",
    "        cache[\"tfidf\"][(token_id, document_id)] = term_tf_idf\n",
    "\n",
    "        vectors[i] = vector * term_tf_idf  # Scale\n",
    "\n",
    "    return doc_centroid(vectors), cache  # Now compute centroid\n",
    "\n",
    "\n",
    "class VectorCollection:\n",
    "    \"\"\" Class to store word embeddings created by Word2Vec. \"\"\"\n",
    "    def __init__(self, word_vectors, context_vectors):\n",
    "        self.word_vectors = {\n",
    "            word: word_vectors.syn0[vocab_item.index] for word, vocab_item in word_vectors.vocab.items()\n",
    "        }  # W_in vectors\n",
    "        self.context_vectors = {\n",
    "            word_vectors.index2word[i]: vec for i, vec in enumerate(context_vectors)\n",
    "        }  # W_out vectors\n",
    "\n",
    "    @staticmethod\n",
    "    def load_vectors(path, to_train=False):\n",
    "        \"\"\"\n",
    "        Load word embeddings created by gensim Word2Vec from a directory.\n",
    "        If the to_train flag is set, the whole model will be returned, otherwise just an instance of VectorCollection\n",
    "        only containing the embeddings to save memory.\n",
    "        \"\"\"\n",
    "        model = Word2Vec.load(path)\n",
    "\n",
    "        if to_train:\n",
    "            return model\n",
    "\n",
    "        # In case it doesn't need to be trained, delete train code to free up ram\n",
    "        word_vectors = model.wv\n",
    "\n",
    "        context_vectors = dict()\n",
    "        if hasattr(model, \"syn1\"):\n",
    "            # For hierarchical softmax\n",
    "            context_vectors = model.syn1\n",
    "        elif hasattr(model, \"syn1neg\"):\n",
    "            # For negative sampling\n",
    "            context_vectors = model.syn1neg\n",
    "\n",
    "        del model  # Save memory\n",
    "        return VectorCollection(word_vectors, context_vectors)\n",
    "\n",
    "\n",
    "def calculate_document_representations(pyndri_index, vector_collection, document_ids, *combination_funcs,\n",
    "                                       vector_func=lambda word, collection: collection.word_vectors[word], **kwargs):\n",
    "    \"\"\"\n",
    "    Pre-compute document vector representations to speed up retrieval later.\n",
    "    :param: pyndri_index: Pyndri index object\n",
    "    :type: pyndri_index: pyndri.index\n",
    "    :param: vector_collection: VectorCollection containing both word and context vectors.\n",
    "    :type: VectorCollection\n",
    "    :param: document_ids: IDs of all the documents inside the index.\n",
    "    :type: set or list or tuple\n",
    "    :param: combination_funcs: Functions that are used to combine word embeddings.\n",
    "    :type: tuple of functions\n",
    "    :param: vector_func: Function used to retrieve an embedding from the vector collection (is useful to determine\n",
    "    whether word or context vectors are being used.)\n",
    "    :type: vector_func: func\n",
    "    \"\"\"\n",
    "    representations = {func.__name__: dict() for func in combination_funcs}  # Create result dict\n",
    "\n",
    "    token2id, id2token, _ = pyndri_index.get_dictionary()\n",
    "    unkowns = set()  # Set of unknown words for which no embedding exists\n",
    "    n_documents = len(document_ids)\n",
    "    cache = dict()  # For some calculations, caching some computations is helpful\n",
    "\n",
    "    for i, document_id in enumerate(document_ids):\n",
    "        print(\"\\r{:.2f} % of documents processed.\".format((i+1)/n_documents*100), end=\"\", flush=True)\n",
    "\n",
    "        _, token_ids = pyndri_index.document(document_id)\n",
    "\n",
    "        token_ids = [token_id for token_id in token_ids if token_id != 0]  # Filter out stop words\n",
    "        vectors = []\n",
    "        for token_id in token_ids:\n",
    "            word = id2token[token_id]\n",
    "            try:\n",
    "                vectors.append(vector_func(word, vector_collection))\n",
    "            except KeyError:\n",
    "                unkowns.add(word)\n",
    "\n",
    "        for func in combination_funcs:\n",
    "            # Create the representations, store them, discard them if the document was empty.\n",
    "            (representation, _), cache = func(\n",
    "                vectors, token_ids=token_ids, document_id=document_id, cache=cache, **kwargs\n",
    "            )\n",
    "            if representation.shape == (0, ): break\n",
    "            representations[func.__name__][document_id] = representation\n",
    "\n",
    "    print(\"\\n{} unknown words encountered.\".format(len(unkowns)))\n",
    "\n",
    "    return representations\n",
    "\n",
    "\n",
    "def save_document_representations(reps, path):\n",
    "    \"\"\" Store word embeddings using the HDF5 file format. \"\"\"\n",
    "    file = h5py.File(path, 'w')\n",
    "\n",
    "    for name, vector_dict in reps.items():\n",
    "        vectors = [vector_dict[i] for i in vector_dict.keys()]\n",
    "        file.create_dataset(name, data=vectors)\n",
    "\n",
    "    file.close()\n",
    "\n",
    "\n",
    "def load_document_representations(path):\n",
    "    \"\"\" Load word embeddings using the HDF5 file format. \"\"\"\n",
    "    return h5py.File(path, \"r\")\n",
    "\n",
    "\n",
    "def run_retrieval_embeddings_So(index, model_name, queries, document_ids, id2token, vector_collection,\n",
    "                                document_representations, tokenized_queries, combination_func, doc2repr=None,\n",
    "                                **resource_params):\n",
    "    \"\"\"\n",
    "    Score a document and a query using the cosine similarity between the document and the query vector\n",
    "    representation. \n",
    "    \"\"\"\n",
    "    def score_func(index, query_id, document_id, **resource_params):\n",
    "        id2token = resource_params[\"id2token\"]\n",
    "        vector_collection = resource_params[\"vector_collection\"]\n",
    "        tokenized_queries = resource_params[\"tokenized_queries\"]\n",
    "        query_token_ids = tokenized_queries[query_id]\n",
    "        doc2repr = resource_params.get(\"doc2repr\", None)\n",
    "        vector_func_query = resource_params.get(\n",
    "            \"vector_func_query\", lambda word, collection: collection.word_vectors[word]\n",
    "        )\n",
    "        query_vectors = []\n",
    "        for query_token_id in query_token_ids:\n",
    "            if query_token_id == 0: continue\n",
    "            try:\n",
    "                query_vectors.append(vector_func_query(id2token[query_token_id], vector_collection))\n",
    "            except KeyError:\n",
    "                # OOV word\n",
    "                continue\n",
    "        if len(query_vectors) == 0:\n",
    "            return -1\n",
    "\n",
    "        query_vector = combination_func(query_vectors, document_id=document_id,\n",
    "                                        token_ids=tokenized_queries[query_id], **resource_params)\n",
    "        query_vector = unpack_vec(query_vector)\n",
    "\n",
    "        try:\n",
    "            lookup_id = document_id if doc2repr is None else doc2repr[document_id]\n",
    "            document_vector = document_representations[lookup_id]\n",
    "            document_vector = unpack_vec(document_vector)\n",
    "        except KeyError as ie:\n",
    "            # Empty documents, give worst score\n",
    "            return -1\n",
    "\n",
    "        return cosine_similarity(query_vector, document_vector)\n",
    "\n",
    "    return run_retrieval(\n",
    "        index, model_name, queries, document_ids, score_func,\n",
    "        # Named key word arguments to build as resources before scoring\n",
    "        id2token=id2token, vector_collection=vector_collection, document_representations=document_representations,\n",
    "        combination_func=combination_func, doc2repr=doc2repr, tokenized_queries=tokenized_queries, **resource_params\n",
    "    )\n",
    "\n",
    "\n",
    "def run_retrieval_embeddings_Savg(index, model_name, queries, document_ids, id2token, vector_collection,\n",
    "                             document_representations, tokenized_queries, doc2repr=None):\n",
    "    \"\"\"\n",
    "    Score a document and a query using the average cosine similarity between the document and the all\n",
    "    of the query word embeddings.\n",
    "    \"\"\"\n",
    "    def score_func(index, query_id, document_id, **resource_params):\n",
    "        id2token = resource_params[\"id2token\"]\n",
    "        vector_collection = resource_params[\"vector_collection\"]\n",
    "        tokenized_queries = resource_params[\"tokenized_queries\"]\n",
    "        doc2repr = resource_params.get(\"doc2repr\", None)\n",
    "        query_token_ids = tokenized_queries[query_id]\n",
    "        vector_func_query = resource_params.get(\n",
    "            \"vector_func_query\", lambda word, collection: collection.word_vectors[word]\n",
    "        )\n",
    "        query_vectors = []\n",
    "        for query_token_id in query_token_ids:\n",
    "            if query_token_id == 0: continue\n",
    "            try:\n",
    "                query_vectors.append(vector_func_query(id2token[query_token_id], vector_collection))\n",
    "            except KeyError:\n",
    "                # OOV word\n",
    "                continue\n",
    "        if len(query_vectors) == 0:\n",
    "            return -1\n",
    "\n",
    "        try:\n",
    "            lookup_id = document_id if doc2repr is None else doc2repr[document_id]\n",
    "            document_vector = document_representations[lookup_id]\n",
    "        except KeyError:\n",
    "            # Empty documents, give worst score\n",
    "            return -1\n",
    "\n",
    "        return sum(\n",
    "            [cosine_similarity(query_vector, document_vector) for query_vector in query_vectors]\n",
    "        ) / len(query_vectors)\n",
    "\n",
    "    return run_retrieval(\n",
    "        index, model_name, queries, document_ids, score_func,\n",
    "        # Named key word arguments to build as resources before scoring\n",
    "        id2token=id2token, vector_collection=vector_collection, document_representations=document_representations,\n",
    "        tokenized_queries=tokenized_queries, doc2repr=doc2repr\n",
    "    )\n",
    "\n",
    "\n",
    "def run_retrieval_plm(index, model_name, queries, document_ids, query_word_positions, background_model,\n",
    "                      tokenized_queries, collection_length, kernel=k_gaussian):\n",
    "    def score_func(index, query_id, document_id, **resource_params):\n",
    "        query_word_positions = resource_params[\"query_word_positions\"]\n",
    "        background_model = resource_params.get(\"background_model\", None)\n",
    "        document_length = index.document_length(document_id)\n",
    "        query_term_positions_for_document = query_word_positions[document_id][int(query_id)]\n",
    "\n",
    "        # If none of the query terms appear in this document, the score is 0\n",
    "        if not query_term_positions_for_document:\n",
    "            return 0\n",
    "\n",
    "        query_term_ids = tokenized_queries[query_id]\n",
    "\n",
    "        plm = PLM(\n",
    "            query_term_ids, document_length, query_term_positions_for_document,\n",
    "            background_model=background_model, kernel=kernel, collection_length=collection_length\n",
    "        )\n",
    "        score = plm.best_position_strategy_score()\n",
    "        print(\"Score\", score)\n",
    "        return score\n",
    "\n",
    "    return run_retrieval(\n",
    "        index, model_name, queries, document_ids, score_func,\n",
    "        # Named key word arguments to build as resources before scoring\n",
    "        query_word_positions=query_word_positions, kernel=kernel, background_model=background_model,\n",
    "        tokenized_queries=tokenized_queries\n",
    "    )\n",
    "\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    \"\"\"Takes 2 vectors a, b and returns the cosine similarity according\n",
    "    to the definition of the dot product\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(a, b)\n",
    "    norm_a = np.linalg.norm(a)\n",
    "    norm_b = np.linalg.norm(b)\n",
    "    res = dot_product / (norm_a * norm_b)\n",
    "    if np.isnan(res):\n",
    "        # If one of the vectors have zero length,\n",
    "        # we can not score the similarity between the two vectors, so we assume the worst\n",
    "        return -1\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def create_document_id_to_repr_map(document_ids):\n",
    "    \"\"\"\n",
    "    Compensate for empty documents in document collections.\n",
    "    Because of using hd5 storage, we can only store vectors of the same length, so no zero-length vectors for empty\n",
    "    documents. Because they were disregarded during the strorage procedure, the alignment between document ids and\n",
    "    indices of the list with their vector representations is now off and has to be fixed.\n",
    "\n",
    "    It's a little dirty and hack-y, but was the best solution given the time constraint.\n",
    "    \"\"\"\n",
    "    empty_ids = {93688, 102435, 104040, 121863, 121866, 122113, 147904, 149905, 153512, 154467, 155654}\n",
    "    doc2repr = dict()\n",
    "\n",
    "    zone = 1\n",
    "    for i in range(1, len(document_ids)+1):\n",
    "        if i in empty_ids:\n",
    "            zone += 1\n",
    "            continue\n",
    "\n",
    "        doc2repr[i] = i - zone\n",
    "\n",
    "    return doc2repr\n",
    "\n",
    "\n",
    "def unpack_vec(vector):\n",
    "    while type(vector) == tuple:\n",
    "        vector = vector[0]\n",
    "\n",
    "    return vector\n",
    "\n",
    "\n",
    "def eval_word_embedding_models(index, queries, document_ids, tf_idf, tokenized_queries, id2df, num_documents,\n",
    "                               document_term_freqs, inverted_index):\n",
    "\n",
    "    # Use tf-idf for re-ranking\n",
    "\n",
    "    run_retrieval(\n",
    "        index, 'tfidf', queries, document_ids, tf_idf,\n",
    "        document_term_freqs=document_term_freqs, tokenized_queries=tokenized_queries, id2df=id2df,\n",
    "        num_documents=num_documents\n",
    "    )\n",
    "    print(\"Reading word embeddings...\")\n",
    "    vectors = VectorCollection.load_vectors(\"./w2v_60\")\n",
    "    doc2repr = create_document_id_to_repr_map(document_ids)\n",
    "    print(\"Reading document representations...\")\n",
    "    doc_representations = load_document_representations(\"./win_representations_1_4\")\n",
    "    # OUT - IN\n",
    "    run_retrieval_embeddings_So(\n",
    "        index, \"embeddings_So_centroid_wout_win\", queries, document_ids, id2token=id2token, vector_collection=vectors,\n",
    "        document_representations=doc_representations.get(\"doc_centroid\"), tokenized_queries=tokenized_queries,\n",
    "        doc2repr=doc2repr, combination_func=doc_centroid,\n",
    "        vector_func_query=lambda word, collection: collection.context_vectors[word],\n",
    "        document_term_freqs=inverted_index, id2df=id2df, number_of_documents=num_documents, cache=dict()\n",
    "    )\n",
    "    del doc_representations\n",
    "    doc_representations = load_document_representations(\"./wout_representations_1_4\")\n",
    "\n",
    "    # IN - OUT\n",
    "    run_retrieval_embeddings_So(\n",
    "        index, \"embeddings_So_centroid_win_wout\", queries, document_ids, id2token=id2token, vector_collection=vectors,\n",
    "        document_representations=doc_representations.get(\"doc_centroid\"), tokenized_queries=tokenized_queries,\n",
    "        doc2repr=doc2repr, combination_func=doc_centroid,\n",
    "        document_term_freqs=inverted_index, id2df=id2df, number_of_documents=num_documents, cache=dict()\n",
    "    )\n",
    "\n",
    "    # OUT - OUT\n",
    "    run_retrieval_embeddings_So(\n",
    "        index, \"embeddings_So_centroid_wout_wout\", queries, document_ids, id2token=id2token, vector_collection=vectors,\n",
    "        document_representations=doc_representations.get(\"doc_centroid\"), tokenized_queries=tokenized_queries,\n",
    "        doc2repr=doc2repr, combination_func=doc_centroid,\n",
    "        vector_func_query=lambda word, collection: collection.context_vectors[word],\n",
    "        document_term_freqs=inverted_index, id2df=id2df, number_of_documents=num_documents, cache=dict()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Learning to rank (LTR) [15 points] (open-ended) ###\n",
    "\n",
    "In this task you will get an introduction into learning to rank for information retrieval.\n",
    "\n",
    "You can explore different ways for devising features for the model. Obviously, you can use the retrieval methods you implemented in Task 1, Task 2 and Task 3 as features. Think about other features you can use (e.g. query/document length). Creativity on devising new features and providing motivation for them will be taken into account when grading.\n",
    "\n",
    "For every query, first create a document candidate set using the top-1000 documents using TF-IDF, and subsequently compute features given a query and a document. Note that the feature values of different retrieval methods are likely to be distributed differently.\n",
    "\n",
    "You are adviced to start some pointwise learning to rank algorithm e.g. logistic regression, implemented in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html).\n",
    "Train your LTR model using 10-fold cross validation on the test set. More advanced learning to rank algorithms will be appreciated when grading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Task 4: Learning to rank (LTR)\n",
    "# ------------------------------\n",
    "def get_document_id_maps(index, document_ids):\n",
    "    id2ext = {}\n",
    "    ext2id = {}\n",
    "\n",
    "    for document_id in document_ids:\n",
    "        ext_doc_id, doc_token_ids = index.document(document_id)\n",
    "        id2ext[document_id] = ext_doc_id\n",
    "        ext2id[ext_doc_id] = document_id\n",
    "\n",
    "    return id2ext, ext2id\n",
    "\n",
    "\n",
    "def get_top_tf_idf_documents(queries, document_ids, index, max_objects_per_query=1000):\n",
    "\n",
    "    top_documents_for_query = collections.defaultdict(list)\n",
    "\n",
    "    id2ext, ext2id = get_document_id_maps(index, document_ids)\n",
    "\n",
    "    with open(\"./lexical_results/tfidf.run\", \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            query_id, _, external_document_id, ranking, score, class_name = line.split()\n",
    "            top_documents_for_query[query_id].append(\\\n",
    "                (float(score), ext2id[external_document_id], external_document_id))\n",
    "\n",
    "\n",
    "    return top_documents_for_query\n",
    "\n",
    "def extract_values_from_run_file(filepath):\n",
    "    # Since we have already calculated a lot of the feature values in previous runs\n",
    "    # we can use these values and only calculate the remaining ones\n",
    "    cache = collections.defaultdict(lambda: collections.defaultdict(float))\n",
    "    with open(filepath, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            query_id, _, document_id, ranking, score, class_name = line.split()\n",
    "            # Append feature values to the features lookup table directly\n",
    "            cache[int(query_id)][document_id] = float(score)\n",
    "\n",
    "    return cache\n",
    "\n",
    "def write_features_to_file(features, filepath):\n",
    "    s = \"\"\n",
    "\n",
    "    for query_id, documents in features.items():\n",
    "        for document_id, feature_vector in documents.items():\n",
    "            s += \"{} {} {}\\n\".format(query_id, document_id, [feature for feature in feature_vector])\n",
    "\n",
    "    with open(filepath, \"w\") as f:\n",
    "        f.write(s)\n",
    "\n",
    "\n",
    "def extract_features(queries, document_ids, index,\\\n",
    "            document_term_freqs, avg_doc_length, id2df, num_documents, tokenized_queries, collection_length):\n",
    "    \"\"\"\n",
    "    Goal: return features[query_id][document_id] = feature_vector\n",
    "\n",
    "    \"\"\"\n",
    "    features = collections.defaultdict(lambda: collections.defaultdict(list))\n",
    "\n",
    "    documents_for_query = get_top_tf_idf_documents(queries, document_ids, index)\n",
    "\n",
    "\n",
    "    bm25_cache = extract_values_from_run_file(\"./lexical_results/bm25.run\")\n",
    "    jm_cache = extract_values_from_run_file(\"./lexical_results/LM_jelinek_mercer_smoothing_0_7.run\")\n",
    "\n",
    "    i = -1\n",
    "    for query_id, documents in documents_for_query.items():\n",
    "        i += 1\n",
    "        print(\"Extracting features for query nr {}\".format(i))\n",
    "        for tf_idf_score, document_id, external_document_id in documents:\n",
    "            ### The different features\n",
    "\n",
    "            # tf-idf\n",
    "            features[int(query_id)][external_document_id].append(float(tf_idf_score))\n",
    "\n",
    "            # bm25\n",
    "            if external_document_id in bm25_cache[query_id]:\n",
    "                score = bm25_cache[query_id][external_document_id]\n",
    "\n",
    "            else:\n",
    "                score = bm25(index, query_id, document_id, document_term_freqs, \\\n",
    "                        avg_doc_length, id2df, num_documents, tokenized_queries)\n",
    "\n",
    "            features[int(query_id)][external_document_id].append(float(score))\n",
    "\n",
    "            # JM\n",
    "            if external_document_id in jm_cache[query_id]:\n",
    "                score = jm_cache[query_id][external_document_id]\n",
    "\n",
    "            else:\n",
    "                score = LM_jelinek_mercer_smoothing(index, query_id, document_id, document_term_freqs, collection_length, tf_C,\n",
    "                                tokenized_queries)\n",
    "            features[int(query_id)][external_document_id].append(float(score))\n",
    "\n",
    "            # Document length\n",
    "            features[int(query_id)][external_document_id].append(index.document_length(document_id))\n",
    "\n",
    "            # Query length\n",
    "            features[int(query_id)][external_document_id].append(len(tokenized_queries[query_id]))\n",
    "\n",
    "    write_features_to_file(features, \"./features.txt\")\n",
    "    return features\n",
    "\n",
    "def evaluate_model(model, test_features, run_out_path, cross_id):\n",
    "    scores = collections.defaultdict(list)\n",
    "\n",
    "    for feature in test_features:\n",
    "        query_id, ext_document_id, feature_vector = feature\n",
    "        feature_vector = np.array(feature_vector)\n",
    "        score = model.predict(feature_vector.reshape(1, -1))\n",
    "        scores[int(query_id)].append((float(score), ext_document_id))\n",
    "\n",
    "    with open('{}{}.run'.format(run_out_path, cross_id), 'w') as f_out:\n",
    "        write_run(\n",
    "            model_name=\"regression\",\n",
    "            data=scores,\n",
    "            out_f=f_out,\n",
    "            max_objects_per_query=1000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    index, token2id, id2token, id2df, dictionary, document_ids = create_index_resources()\n",
    "    num_documents = len(document_ids)\n",
    "\n",
    "    queries, tokenized_queries, query_terms_inverted, query_term_ids = create_query_resources()\n",
    "\n",
    "    inverted_index, tf_C, query_word_positions, unique_terms_per_document, avg_doc_length, document_length, \\\n",
    "        collection_length = build_misc_resources(document_ids, query_terms_inverted)\n",
    "    document_term_freqs = inverted_index\n",
    "\n",
    "    create_all_lexical_run_files(\n",
    "        index, document_ids, queries, document_term_freqs, collection_length, tf_C,\n",
    "        tokenized_queries, background_model=tf_C, idf2df=id2df, num_documents=num_documents\n",
    "    )\n",
    "    features = extract_features(queries, document_ids, index,\\\n",
    "            document_term_freqs, avg_doc_length, id2df, num_documents, tokenized_queries, collection_length)\n",
    "\n",
    "    datasets, test_features = zip(*[get_dataset_for_features(features, i=i) for i in range(10)])\n",
    "    test_features = test_features[0]\n",
    "\n",
    "    id2ext, ext2id = get_document_id_maps(index, document_ids)\n",
    "\n",
    "    for i, dataset in enumerate(datasets):\n",
    "        model = train(dataset)\n",
    "        evaluate_model(model, test_features, \"./regression\", i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Task 4: Write a report [15 points; instant FAIL if not provided] ###\n",
    "\n",
    "The report should be a PDF file created using the [sigconf ACM template](https://www.acm.org/publications/proceedings-template) and will determine a significant part of your grade.\n",
    "\n",
    "   * It should explain what you have implemented, motivate your experiments and detail what you expect to learn from them. **[10 points]**\n",
    "   * Lastly, provide a convincing analysis of your results and conclude the report accordingly. **[10 points]**\n",
    "      * Do all methods perform similarly on all queries? Why?\n",
    "      * Is there a single retrieval model that outperforms all other retrieval models (i.e., silver bullet)?\n",
    "      * ...\n",
    "\n",
    "**Hand in the report and your self-contained implementation source files.** Only send us the files that matter, organized in a well-documented zip/tgz file with clear instructions on how to reproduce your results. That is, we want to be able to regenerate all your results with minimal effort. You can assume that the index and ground-truth information is present in the same file structure as the one we have provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
