{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Information retrieval --- Module 1: Evaulation\n",
    "Santhosh Kumar Rajamanickam, Dennis Ulmer and Stian Steinbakken\n",
    "\n",
    "TODO: Maybe insert a small introduction here? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical part [15pts]\n",
    "### 1 Hypothesis Testing --- The problem of multiple comparisons. \n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = Œ±):\n",
    "1. P(mth experiment gives significant result | m experiments lacking power to reject H0)?\n",
    "2. P(at least one significant result | m experiments lacking power to reject H0)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "1. Because the outcome of the experiments are independent, we can just multiply the probabilities of the outcomes in this question. The probability of an experiment giving a significant result, i.e. rejecting the Null Hypothesis, will be $\\alpha$. Likewise, the probability of experiments lacking the power to reject $H_0$ will be $1-\\alpha$. Therefore\n",
    "\n",
    "\\begin{equation}\n",
    "P(m^{th}\\text{ experiment gives significant result } | \\ m\\text{ experiments lacking power to reject } H_0) = (1-\\alpha)^{m-1}\\alpha\n",
    "\\end{equation}\n",
    "\n",
    "2. The probability of at least one in $m$ experiments rejecting the Null Hypothesis falsely is the sum of the probabilities of a experiment at time step $t$ rejecting the Null-Hypothesis conditioned on the previous experiments rejecting it correctly:\n",
    "\n",
    "\\begin{equation}\n",
    "   p(\\text{rejecting the Null Hypothesis at least once}|m\\text{ experiments lacking power to reject }H_0)\\\\ = p(1^{st}\\text{ wrong}) + p(2^{nd} \\text{wrong}\\ |\\ 1^{st}\\ \\text{right}) + \\ldots + p(m^{th} \\text{wrong}\\ |\\ \\text{all}\\ m-1\\ \\text{experiments}\\ \\text{right})\\\\\n",
    "   = \\alpha + (\\alpha - 1)\\alpha + (\\alpha-1)^2\\alpha + \\ldots + (\\alpha-1)^{m-1}\\alpha\\\\\n",
    "   = \\alpha \\cdot {\\displaystyle \\sum_{t=0}^{m-1}} (\\alpha-1)^t\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning ‚Öî of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**\n",
    "\n",
    "Assume we have to algorithms A and B providing documents for an interleaved ranking of length $3$, where A is the worse algorithm. While constructing the interleaved ranking, the result of two coin tosses where A wins gives A preference to place its documents first. Because we only have three entries, two of the entries will originate from A and only one from B.\n",
    "\n",
    "Given random clicking, A will again win ‚Öî of times. Here the unfairness is due to the coin tosses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental part [85 pts] \n",
    "Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "P: {N N N N N}\n",
    "E: {N N N N R}\n",
    "‚Ä¶\n",
    "P: {HR HR HR HR R}\n",
    "E: {HR HR HR HR HR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59049 simulations in total.\n",
      "\n",
      "First 10 simulations:\n",
      "\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'N', 'N')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'N', 'R')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'N', 'HR')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'R', 'N')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'R', 'R')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'R', 'HR')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'HR', 'N')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'HR', 'R')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'HR', 'HR')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'R', 'N', 'N')\n",
      "\n",
      "Last 10 simulations:\n",
      "\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'R', 'HR', 'HR')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'N', 'N')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'N', 'R')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'N', 'HR')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'R', 'N')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'R', 'R')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'R', 'HR')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'HR', 'N')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'HR', 'R')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'HR', 'HR')\n"
     ]
    }
   ],
   "source": [
    "# Step 1 code\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "\n",
    "RankingPair = namedtuple(\"RankingPair\", [\"E\", \"P\"])\n",
    "\n",
    "RANKINGS = (\"N\", \"R\", \"HR\")\n",
    "\n",
    "simulations = []\n",
    "\n",
    "# Create all possible rankings of length 5\n",
    "for ranking_pair in list(product(list(product(RANKINGS, repeat=5)), repeat=2)):\n",
    "    simulations.append(RankingPair(*ranking_pair))\n",
    "\n",
    "print(\"{} simulations in total.\\n\".format(len(simulations)))\n",
    "    \n",
    "print(\"First 10 simulations:\\n\")\n",
    "for ranking_pair in simulations[:10]:\n",
    "    print(\"P: {}, E: {}\".format(*ranking_pair))\n",
    "    \n",
    "print(\"\\nLast 10 simulations:\\n\")\n",
    "for ranking_pair in simulations[-10:]:\n",
    "    print(\"P: {}, E: {}\".format(*ranking_pair))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2: Implement Evaluation Measures (10 points)\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All average-precision tests passed\n"
     ]
    }
   ],
   "source": [
    "# Step 2 code\n",
    "# TODO: Dennis: implement 1 multi-graded\n",
    "# TODO: Santosh: implement 1 multi-graded\n",
    "\n",
    "def precision(rankings):\n",
    "    relevant_tags = (RANKINGS[1], RANKINGS[2]) # 'R' and 'HR'\n",
    "    \n",
    "    relevant_rankings = sum([1 for rank in rankings if rank in relevant_tags])\n",
    "    return relevant_rankings / len(rankings) \n",
    "\n",
    "# For our binary evaulation meassure we implement average precision (AP)\n",
    "def average_precision(rankings):\n",
    "    relevant_tags = (RANKINGS[1], RANKINGS[2]) # 'R' and 'HR'\n",
    "    return sum([precision(rankings[:i+1]) for i in range(len(rankings)) if rankings[i] in relevant_tags]) / len(rankings)\n",
    "\n",
    "def test_average_precision():\n",
    "    example_rank_1 = ('R', 'R', 'R', 'R', 'R')\n",
    "    example_rank_2 = ('N', 'N', 'N', 'N', 'N')\n",
    "    example_rank_3 = ('N', 'R', 'N', 'N', 'R')\n",
    "    \"\"\"\n",
    "    Example rank 3 should yield (1/2 + 2/5) / 5 = (0.5 + 0.4) / 5 = 0.18\n",
    "    \"\"\"\n",
    "    \n",
    "    assert average_precision(example_rank_1) == 1\n",
    "    assert average_precision(example_rank_2) == 0\n",
    "    assert average_precision(example_rank_3) == 0.18\n",
    "    return True\n",
    "\n",
    "AP_passed = test_average_precision()\n",
    "if AP_passed: print(\"All average-precision tests passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "import math\n",
    "\n",
    "# Assumptation\n",
    "RANKING_WEIGHTS = {\"N\": 0, \"R\": 1, \"HR\": 5}\n",
    "\n",
    "def ndcg(ranks, k=5):\n",
    "    return sum([(2**RANKING_WEIGHTS[rank]-1) / math.log(2+i, 2) for i, rank in enumerate(ranks[:k])])\n",
    "\n",
    "def evaluate(ranking_pairs, metric, **metric_args):\n",
    "    scores = []\n",
    "    \n",
    "    for ranking_pair in ranking_pairs:\n",
    "        scores.append((metric(ranking_pair.E, **metric_args), metric(ranking_pair.P, **metric_args)))\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the ùõ•measure (10 points)\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ùõ•measure = measureE-measureP. Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta_m for average_precision: 8967.593333333285\n",
      "Delta_m for Normalized Discounted Cumulative Gain: 677954.3127262632\n"
     ]
    }
   ],
   "source": [
    "# Step 3 code\n",
    "# TODO: Stian\n",
    "\n",
    "def delta_measure(simulations, measure_to_test):\n",
    "    # Measure_to_test argument must be a function that takes in a ranking to be measured\n",
    "    # We do this so that the function can be re-used for each of the measures\n",
    "    \n",
    "    delta_m = 0\n",
    "    for sim in simulations:\n",
    "        E_ranking, P_ranking = sim\n",
    "        P_measure, E_measure = measure_to_test(P_ranking), measure_to_test(E_ranking)\n",
    "        \n",
    "        # TODO: I am unsure if we are supposed to accumulate the delta, or only consider each simulation by itself\n",
    "        # Currently accumulating\n",
    "        delta_m += E_measure - P_measure if E_measure > P_measure else 0\n",
    "    return delta_m\n",
    "        \n",
    "delta_m_AP = delta_measure(simulations, average_precision)\n",
    "delta_m_ndcg = delta_measure(simulations, ndcg)\n",
    "print(\"Delta_m for average_precision:\", delta_m_AP)\n",
    "print(\"Delta_m for Normalized Discounted Cumulative Gain:\", delta_m_ndcg)\n",
    "\n",
    "# TODO: Run delta_measure for the remaining measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Interleaving (15 points)\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'N', 'R', 'N', 'R']\n"
     ]
    }
   ],
   "source": [
    "# Step 4 code\n",
    "# TODO: Stian (I can do both)\n",
    "from random import choice \n",
    "\n",
    "def team_draft_interleaving(rankings_1, rankings_2, interleaved_rankings_size=5):\n",
    "    interleaved_rankings = []\n",
    "    i = -1\n",
    "    while len(interleaved_rankings) < interleaved_rankings_size:\n",
    "        i += 1\n",
    "        \n",
    "        if choice([\"H\", \"T\"]) == \"H\":\n",
    "            interleaved_rankings.append(rankings_1[i])\n",
    "            \n",
    "            if len(interleaved_rankings) < interleaved_rankings_size:\n",
    "                interleaved_rankings.append(rankings_2[i])\n",
    "        else:\n",
    "            interleaved_rankings.append(rankings_2[i])\n",
    "            \n",
    "            if len(interleaved_rankings) < interleaved_rankings_size:\n",
    "                interleaved_rankings.append(rankings_1[i])\n",
    "            \n",
    "    return interleaved_rankings\n",
    "\n",
    "example_rank_1 = ('R', 'R', 'R', 'R', 'R')\n",
    "example_rank_2 = ('N', 'N', 'N', 'N', 'N')\n",
    "\n",
    "interleaved = team_draft_interleaving(example_rank_1, example_rank_2)\n",
    "print(interleaved)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement User Clicks Simulation (15 points)\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "\n",
    "We have considered a number of click models including:\n",
    "1. Random Click Model (RCM)\n",
    "2. Position-Based Model (PBM)\n",
    "3. Simple Dependent Click Model (SDCM)\n",
    "4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<YandexData comprising 11716 sessions> \n",
      "<Session #5>\n",
      "<Session #6>\n",
      "<Session #7>\n",
      "<Session #8>\n",
      "<Session #9>\n",
      "\n",
      "<Query #0 in session #0>\n",
      "<Query #0 in session #0>\n",
      "<Query #0 in session #0>\n",
      "<Query #0 in session #0>\n",
      "<Query #0 in session #0>\n",
      "\n",
      "<Click in session #0 on #()>\n",
      "<Click in session #0 on #()>\n",
      "<Click in session #0 on #()>\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "\n",
    "class YandexData:\n",
    "    \"\"\" Class to represent the entire dataset. \"\"\"\n",
    "    def __init__(self, sessions=[]):\n",
    "        self.sessions = sessions\n",
    "        \n",
    "    def add(self, session):\n",
    "        self.sessions.append(session)\n",
    "        \n",
    "    def __getitem__(self, items):\n",
    "        return self.sessions[items]\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (session for session in self.sessions)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sessions)\n",
    "    \n",
    "    @property\n",
    "    def queries(self):\n",
    "        \"\"\" Return all queries in the entire data set. \"\"\"\n",
    "        for session in self:\n",
    "            for query in session.queries:\n",
    "                yield query\n",
    "                \n",
    "    @property\n",
    "    def clicks(self):\n",
    "        \"\"\" Return all clicks in the entire data set. \"\"\"\n",
    "        for session in self:\n",
    "            for click in session.clicks:\n",
    "                yield click\n",
    "                \n",
    "    def __repr__(self):\n",
    "        return \"<YandexData comprising {} sessions>\".format(len(self))\n",
    "\n",
    "\n",
    "class Session:\n",
    "    def __init__(self, uid, user_actions):\n",
    "        self.uid = uid\n",
    "        self.user_actions = user_actions\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (user_action for user_action in self.user_actions)\n",
    "    \n",
    "    @property\n",
    "    def queries(self):\n",
    "        \"\"\" Return all queries in this session. \"\"\"\n",
    "        return (action for action in self.user_actions if action.action_type == \"Q\")\n",
    "    \n",
    "    @property\n",
    "    def clicks(self):\n",
    "        \"\"\" Return all clicks in this session. \"\"\"\n",
    "        return (action for action in self.user_actions if action.action_type == \"C\")\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Session #{}>\".format(self.uid)\n",
    "        \n",
    "\n",
    "class UserAction:\n",
    "    def __init__(self, session_id, time_passed, action_type, *document_ids):\n",
    "        self.session_id = session_id\n",
    "        self.time_passed = time_passed\n",
    "        self.action_type = action_type\n",
    "        self.document_ids = document_ids\n",
    "        \n",
    "    def __iter__(self):\n",
    "        return (document_id for document_id in self.document_ids)\n",
    "            \n",
    "    def __contains__(self, document_id):\n",
    "        return document_id in self.document_ids\n",
    "        \n",
    "class Query(UserAction):\n",
    "    def __init__(self, session_id, time_passed, serp_id, query_id, *document_ids):\n",
    "        self.query_id = query_id\n",
    "        self.serp_id = serp_id\n",
    "        super().__init__(session_id, time_passed, \"Q\", serp_id, *document_ids)\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"<Query #{} in session #{}>\".format(self.query_id, self.session_id)\n",
    "        \n",
    "        \n",
    "class Click(UserAction):\n",
    "    def __init__(self, session_id, time_passed, *document_ids):\n",
    "        super().__init__(session_id, time_passed, \"C\", *document_ids)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"<Click in session #{} on #{}>\".format(self.session_id, self.document_ids)\n",
    "        \n",
    "\n",
    "\n",
    "def read_yandex_data(path=\"./YandexRelPredChallenge.txt\"):\n",
    "    \"\"\" Parse the data set into an appropriate data structure. \"\"\"\n",
    "    data = YandexData()\n",
    "    current_session_id = 0\n",
    "    current_actions = []\n",
    "    \n",
    "    with codecs.open(path, \"rb\", \"utf-8\") as file:\n",
    "        for line in file.readlines():\n",
    "            columns = line.strip().split(\"\\t\")\n",
    "            columns = tuple(map(lambda x: int(x) if x not in (\"Q\", \"C\") else x, columns))  # Parse numbers\n",
    "            session_id, time_passed, action_type, *document_ids = columns\n",
    "            \n",
    "            # New session, create object, save it and start a new one\n",
    "            if session_id != current_session_id:\n",
    "                session = Session(current_session_id, current_actions)\n",
    "                data.add(session)\n",
    "                \n",
    "                current_session_id = session_id\n",
    "                current_actions = []\n",
    "            \n",
    "            if action_type == \"Q\":\n",
    "                # Get query id\n",
    "                query_id, serp_id, *document_ids = document_ids\n",
    "                current_actions.append(Query(session_id, time_passed, serp_id, query_id, *document_ids))\n",
    "            else:\n",
    "                # It's a click!\n",
    "                current_actions.append(Click(session_id, time_passed,  *document_ids))\n",
    "                \n",
    "    return data\n",
    "        \n",
    "yandex_data = read_yandex_data()\n",
    "print(yandex_data, \"\\n\")\n",
    "\n",
    "# You can now iterate over sessions in the data set like this:\n",
    "for session in yandex_data:\n",
    "    pass  # Do something\n",
    "\n",
    "# You iterate over slices of the data set:\n",
    "for session in yandex_data[5:10]:\n",
    "    print(session)\n",
    "    \n",
    "print(\"\")\n",
    "    \n",
    "# You can iterate over all the queries in the entire data set:\n",
    "for query in yandex_data.queries:\n",
    "    pass  # Do something\n",
    "\n",
    "# ... or of a single session\n",
    "for query in yandex_data[0].queries:\n",
    "    print(query)\n",
    "    \n",
    "# (Multiple queries with the same id might be the user clicking trough result pages?)\n",
    "    \n",
    "print(\"\")\n",
    "    \n",
    "for click in yandex_data[0].clicks:\n",
    "    print(click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5 code\n",
    "# TODO: Santhosh: implement RCM\n",
    "# TODO: Dennis: one of the remaining 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ClickModel:\n",
    "    \n",
    "    def simulate_clicks(ranking):\n",
    "        \"\"\" Let the model determine the clicks given a ranking. \"\"\"\n",
    "        pass\n",
    "\n",
    "class PositionBasedModel(ClickModel):\n",
    "    def __init__(self, click_prob, examination_prob, attractiveness_prob):\n",
    "        p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter ùëéuq. Use the relevance label to assign this parameter by setting ùëéuq for a document u in the ranked list accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Stian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Results and Analysis (30 points)\n",
    "Compare the results of the offline experiments (i.e. the values of the ùõ•measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7 code and markdown\n",
    "# TODO: we should probably all contribute to this part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
