{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Information retrieval --- Module 1: Evaulation\n",
    "Santosh Kumar Rajamanickam, Dennis Ulmer and Stian Steinbakken\n",
    "\n",
    "TODO: Maybe insert a small introduction here? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theoretical part [15pts]\n",
    "### 1 Hypothesis Testing --- The problem of multiple comparisons. \n",
    "How many hypothesis tests, m, does it take to get to (with Type I error for each test = Î±):\n",
    "1. P(mth experiment gives significant result | m experiments lacking power to reject H0)?\n",
    "2. P(at least one significant result | m experiments lacking power to reject H0)?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: TODO: Santosh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Bias and unfairness in Interleaving experiments [10 points]\n",
    "Balance interleaving has been shown to be biased in a number of corner cases. An example was given during the lecture with two ranked lists of length 3 being interleaved, and a randomly clicking population of users that resulted in algorithm A winning â…” of the time, even though in theory the percentage of wins should be 50% for both algorithms. Can you come up with a situation of two ranked lists of length 3 and a distribution of clicks over them for which Team-draft interleaving is unfair to the better algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer: TODO: Stian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental part [85 pts] \n",
    "Step 1: Simulate Rankings of Relevance for E and P (5 points)\n",
    "In the first step you will generate pairs of rankings of relevance, for the production P and experimental E, respectively, for a hypothetical query q. Assume a 3-graded relevance, i.e. {N, R, HR}. Construct all possible P and E ranking pairs of length 5. This step should give you about.\n",
    "\n",
    "Example:\n",
    "P: {N N N N N}\n",
    "E: {N N N N R}\n",
    "â€¦\n",
    "P: {HR HR HR HR R}\n",
    "E: {HR HR HR HR HR}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59049 simulations in total.\n",
      "\n",
      "First 10 simulations:\n",
      "\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'N', 'N')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'N', 'R')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'N', 'HR')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'R', 'N')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'R', 'R')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'R', 'HR')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'HR', 'N')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'HR', 'R')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'N', 'HR', 'HR')\n",
      "P: ('N', 'N', 'N', 'N', 'N'), E: ('N', 'N', 'R', 'N', 'N')\n",
      "\n",
      "Last 10 simulations:\n",
      "\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'R', 'HR', 'HR')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'N', 'N')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'N', 'R')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'N', 'HR')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'R', 'N')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'R', 'R')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'R', 'HR')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'HR', 'N')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'HR', 'R')\n",
      "P: ('HR', 'HR', 'HR', 'HR', 'HR'), E: ('HR', 'HR', 'HR', 'HR', 'HR')\n"
     ]
    }
   ],
   "source": [
    "# Step 1 code\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple\n",
    "\n",
    "RankingPair = namedtuple(\"RankingPair\", [\"E\", \"P\"])\n",
    "\n",
    "RANKINGS = (\"N\", \"R\", \"HR\")\n",
    "\n",
    "simulations = []\n",
    "\n",
    "# Create all possible rankings of length 5\n",
    "for ranking_pair in list(product(list(product(RANKINGS, repeat=5)), repeat=2)):\n",
    "    simulations.append(RankingPair(*ranking_pair))\n",
    "\n",
    "print(\"{} simulations in total.\\n\".format(len(simulations)))\n",
    "    \n",
    "print(\"First 10 simulations:\\n\")\n",
    "for ranking_pair in simulations[:10]:\n",
    "    print(\"P: {}, E: {}\".format(*ranking_pair))\n",
    "    \n",
    "print(\"\\nLast 10 simulations:\\n\")\n",
    "for ranking_pair in simulations[-10:]:\n",
    "    print(\"P: {}, E: {}\".format(*ranking_pair))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Step 2: Implement Evaluation Measures (10 points)\n",
    "Implement 1 binary and 2 multi-graded evaluation measures out of the 7 measures mentioned above. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All average-precision tests passed\n"
     ]
    }
   ],
   "source": [
    "# Step 2 code\n",
    "# TODO: Dennis: implement 1 multi-graded\n",
    "# TODO: Santosh: implement 1 multi-graded\n",
    "\n",
    "def precision(rankings):\n",
    "    relevant_tags = (RANKINGS[1], RANKINGS[2]) # 'R' and 'HR'\n",
    "    \n",
    "    relevant_rankings = sum([1 for rank in rankings if rank in relevant_tags])\n",
    "    return relevant_rankings / len(rankings) \n",
    "\n",
    "# For our binary evaulation meassure we implement average precision (AP)\n",
    "def average_precision(rankings):\n",
    "    relevant_tags = (RANKINGS[1], RANKINGS[2]) # 'R' and 'HR'\n",
    "    return sum([precision(rankings[:i+1]) for i in range(len(rankings)) if rankings[i] in relevant_tags]) / len(rankings)\n",
    "\n",
    "def test_average_precision():\n",
    "    example_rank_1 = ('R', 'R', 'R', 'R', 'R')\n",
    "    example_rank_2 = ('N', 'N', 'N', 'N', 'N')\n",
    "    example_rank_3 = ('N', 'R', 'N', 'N', 'R')\n",
    "    \"\"\"\n",
    "    Example rank 3 should yield (1/2 + 2/5) / 5 = (0.5 + 0.4) / 5 = 0.18\n",
    "    \"\"\"\n",
    "    \n",
    "    assert average_precision(example_rank_1) == 1\n",
    "    assert average_precision(example_rank_2) == 0\n",
    "    assert average_precision(example_rank_3) == 0.18\n",
    "    return True\n",
    "\n",
    "AP_passed = test_average_precision()\n",
    "if AP_passed: print(\"All average-precision tests passed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Implementation of Normalized Discounted Cumulative Gain at rank k (nDCG@k)\n",
    "import math\n",
    "\n",
    "# Assumptation\n",
    "RANKING_WEIGHTS = {\"N\": 0, \"R\": 1, \"HR\": 5}\n",
    "\n",
    "def ndcg(ranks, k=5):\n",
    "    return sum([(2**RANKING_WEIGHTS[rank]-1) / math.log(2+i, 2) for i, rank in enumerate(ranks[:k])])\n",
    "\n",
    "def evaluate(ranking_pairs, metric, **metric_args):\n",
    "    scores = []\n",
    "    \n",
    "    for ranking_pair in ranking_pairs:\n",
    "        scores.append((metric(ranking_pair.E, **metric_args), metric(ranking_pair.P, **metric_args)))\n",
    "    \n",
    "    return scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Calculate the ð›¥measure (10 points)\n",
    "For the three measures and all P and E ranking pairs constructed above calculate the difference: ð›¥measure = measureE-measureP. Consider only those pairs for which E outperforms P.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta_m for average_precision: 8967.593333333285\n",
      "Delta_m for Normalized Discounted Cumulative Gain: 677954.3127262632\n"
     ]
    }
   ],
   "source": [
    "# Step 3 code\n",
    "# TODO: Stian\n",
    "\n",
    "def delta_measure(simulations, measure_to_test):\n",
    "    # Measure_to_test argument must be a function that takes in a ranking to be measured\n",
    "    # We do this so that the function can be re-used for each of the measures\n",
    "    \n",
    "    delta_m = 0\n",
    "    for sim in simulations:\n",
    "        E_ranking, P_ranking = sim\n",
    "        P_measure, E_measure = measure_to_test(P_ranking), measure_to_test(E_ranking)\n",
    "        \n",
    "        # TODO: I am unsure if we are supposed to accumulate the delta, or only consider each simulation by itself\n",
    "        # Currently accumulating\n",
    "        delta_m += E_measure - P_measure if E_measure > P_measure else 0\n",
    "    return delta_m\n",
    "        \n",
    "delta_m_AP = delta_measure(simulations, average_precision)\n",
    "delta_m_ndcg = delta_measure(simulations, ndcg)\n",
    "print(\"Delta_m for average_precision:\", delta_m_AP)\n",
    "print(\"Delta_m for Normalized Discounted Cumulative Gain:\", delta_m_ndcg)\n",
    "\n",
    "# TODO: Run delta_measure for the remaining measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Implement Interleaving (15 points)\n",
    "Implement 2 interleaving algorithms: (1) Team-Draft Interleaving OR Balanced Interleaving, AND (2), Probabilistic Interleaving. The interleaving algorithms should (a) given two rankings of relevance interleave them into a single ranking, and (b) given the users clicks on the interleaved ranking assign credit to the algorithms that produced the rankings.\n",
    "\n",
    "(Note 4: Note here that as opposed to a normal interleaving experiment where rankings consists of urls or docids, in our case the rankings consist of relevance labels. Hence in this case (a) you will assume that E and P return different documents, (b) the interleaved ranking will also be a ranking of labels.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['N', 'R', 'R', 'N', 'R', 'N']\n"
     ]
    }
   ],
   "source": [
    "# Step 4 code\n",
    "# TODO: Stian (I can do both)\n",
    "from random import choice \n",
    "\n",
    "def team_draft_interleaving(rankings_1, rankings_2, interleaved_rankings_size=5):\n",
    "    interleaved_rankings = []\n",
    "    i = -1\n",
    "    while len(interleaved_rankings) < interleaved_rankings_size:\n",
    "        i += 1\n",
    "        \n",
    "        if choice([\"H\", \"T\"]) == \"H\":\n",
    "            interleaved_rankings.append(rankings_1[i])\n",
    "            interleaved_rankings.append(rankings_2[i])\n",
    "        else:\n",
    "            interleaved_rankings.append(rankings_2[i])\n",
    "            interleaved_rankings.append(rankings_1[i])\n",
    "            \n",
    "    return interleaved_rankings\n",
    "\n",
    "example_rank_1 = ('R', 'R', 'R', 'R', 'R')\n",
    "example_rank_2 = ('N', 'N', 'N', 'N', 'N')\n",
    "\n",
    "interleaved = team_draft_interleaving(example_rank_1, example_rank_2)\n",
    "print(interleaved)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Implement User Clicks Simulation (15 points)\n",
    "Having interleaved all the ranking pairs an online experiment could be ran. However, given that we do not have any users (and the entire homework is a big simulation) we will simulate user clicks.\n",
    "\n",
    "We have considered a number of click models including:\n",
    "1. Random Click Model (RCM)\n",
    "2. Position-Based Model (PBM)\n",
    "3. Simple Dependent Click Model (SDCM)\n",
    "4. Simple Dynamic Bayesian Network (SDBN)\n",
    "\n",
    "Consider two different click models, (a) the Random Click Model (RCM), and (b) one out of the remaining 3 aforementioned models. The parameters of some of these models can be estimated using the Maximum Likelihood Estimation (MLE) method, while others require using the Expectation-Maximization (EM) method. Implement the two models so that (a) there is a method that learns the parameters of the model given a set of training data, (b) there is a method that predicts the click probability given a ranked list of relevance labels, (c) there is a method that decides - stochastically - whether a document is clicked based on these probabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 5 code\n",
    "# TODO: Santosh: implement RCM\n",
    "# TODO: Dennis: one of the remaining 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Simulate Interleaving Experiment (10 points)\n",
    "Having implemented the click models, it is time to run the simulated experiment.\n",
    "\n",
    "For each of interleaved ranking run N simulations for each one of the click models implemented and measure the proportion p of wins for E.\n",
    "(Note 7: Some of the models above include an attractiveness parameter ð‘Žuq. Use the relevance label to assign this parameter by setting ð‘Žuq for a document u in the ranked list accordingly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Stian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Results and Analysis (30 points)\n",
    "Compare the results of the offline experiments (i.e. the values of the ð›¥measure) with the results of the online experiment (i.e. proportion of wins), analyze them and reach your conclusions regarding their agreement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 7 code and markdown\n",
    "# TODO: we should probably all contribute to this part"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
